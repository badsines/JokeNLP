{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_json\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df = read_json(\"reddit_jokes.json\")\n",
    "sentences = df['title'] + ' ' + df['body']\n",
    "# this analyzer from sklearn tokenizes including stop words and lower case\n",
    "analyze = CountVectorizer(stop_words='english').build_analyzer()\n",
    "sentences = [analyze(s) for s in sentences.tolist()]\n",
    "# this trains a cbow on sentences\n",
    "model = word2vec.Word2Vec(sentences, size=100, min_count=10, workers=4)\n",
    "model.wv.save('reddit_jokes.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the non-trainable part of the model from disk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load('reddit_jokes.model')\n",
    "google_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some similarity comparisons between Google's massive model and ours.  Ours does surprisingly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "most_similar(positive=['king'])\n",
      "Trained by Jokes\tTrained by Google\n",
      "queen\t0.74\t\tkings\t0.71\n",
      "arthur\t0.72\t\tqueen\t0.65\n",
      "lancelot\t0.65\t\tmonarch\t0.64\n",
      "kings\t0.65\t\tcrown_prince\t0.62\n",
      "================================================================================\n",
      "most_similar(positive=['king','woman'])\n",
      "Trained by Jokes\tTrained by Google\n",
      "queen\t0.63\t\tman\t0.66\n",
      "arthur\t0.57\t\tqueen\t0.64\n",
      "beauty\t0.55\t\tgirl\t0.61\n",
      "bodyguard\t0.53\t\tprincess\t0.61\n",
      "================================================================================\n",
      "most_similar(positive=['king','woman'], negative=['man'])\n",
      "Trained by Jokes\tTrained by Google\n",
      "queen\t0.65\t\tqueen\t0.71\n",
      "elizabeth\t0.59\t\tmonarch\t0.62\n",
      "birbal\t0.57\t\tprincess\t0.59\n",
      "arthur\t0.55\t\tcrown_prince\t0.55\n",
      "================================================================================\n",
      "most_similar(positive=['man'])\n",
      "Trained by Jokes\tTrained by Google\n",
      "mans\t0.67\t\twoman\t0.77\n",
      "gentleman\t0.66\t\tboy\t0.68\n",
      "guy\t0.66\t\tteenager\t0.66\n",
      "woman\t0.62\t\tteenage_girl\t0.61\n",
      "================================================================================\n",
      "most_similar(positive=['woman'])\n",
      "Trained by Jokes\tTrained by Google\n",
      "lady\t0.75\t\tman\t0.77\n",
      "man\t0.62\t\tgirl\t0.75\n",
      "women\t0.62\t\tteenage_girl\t0.73\n",
      "gentleman\t0.58\t\tteenager\t0.63\n"
     ]
    }
   ],
   "source": [
    "#w = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "#g = google_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "def Display(s):\n",
    "    w = eval('word_vectors.' + s)\n",
    "    g = eval('google_vectors.' + s)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    print ('{}'.format(s))\n",
    "    print('Trained by Jokes\\tTrained by Google')\n",
    "    for i in range(4):  #range(len(w)):\n",
    "        print(\"{}\\t{:4.2}\\t\\t{}\\t{:4.2}\".format(w[i][0], w[i][1], g[i][0], g[i][1]))\n",
    "tests = [\"most_similar(positive=['king'])\", \n",
    "         \"most_similar(positive=['king','woman'])\",\n",
    "         \"most_similar(positive=['king','woman'], negative=['man'])\",\n",
    "         \"most_similar(positive=['man'])\",\n",
    "         \"most_similar(positive=['woman'])\",\n",
    "        ]\n",
    "for s in tests:\n",
    "    print('='*80)\n",
    "    Display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of our vectorization of \"king\" (compare to the ctor for Word2Vec): 100\n",
      "First 12 features of \"king\":  [-1.53627563 -1.44087207  0.02402437  1.61712003 -0.89577299 -0.04609798\n",
      " -1.54079127  1.01438117  0.84849453  0.80059016  1.14108241 -1.77830291]\n",
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "man, 1.98347747326, 1.96043431759, \n",
      "says, 1.17057991028, 0.387207120657, \n",
      "said, 1.29437792301, -1.11761200428, \n",
      "did, -0.731746554375, -1.54007816315, \n",
      "like, 0.520845174789, -0.422606885433, \n",
      "just, 0.421174943447, 2.01930809021, \n",
      "don, -0.217230424285, 1.85953974724, \n",
      "know, 2.2788233757, 0.290828645229, \n",
      "guy, 2.4918589592, 2.01255893707, \n",
      "asks, 2.19117808342, 0.256881028414, \n",
      "The vocabulary is stored as a standard <type 'dict'> of len 20012\n",
      "There is also a <type 'list'>, also of len 20012\n",
      "Here are the first 10 entries:  [u'man', u'says', u'said', u'did', u'like', u'just', u'don', u'know', u'guy', u'asks']\n"
     ]
    }
   ],
   "source": [
    "# Show me the vectorization of the word 'king'\n",
    "vKing = word_vectors['king']\n",
    "print ('Length of our vectorization of \"king\" (compare to the ctor for Word2Vec): {}'.format(len(vKing)))\n",
    "print ('First 12 features of \"king\":  {}'.format(vKing[:12]))\n",
    "# Show me the vocabulary itself# Export to CSV?  Real close here but I don't think we want to go this way.\n",
    "# Instead, use the gensim API / \"KeyedVector\" (word_vector here).\n",
    "from __future__ import print_function\n",
    "def dump_embedding():\n",
    "    for word in word_vectors.index2word[:10]:\n",
    "        print ('{}, '.format(word), end='')\n",
    "        for feat in word_vectors[word][:2]:\n",
    "            print('{}, '.format(feat), end='')\n",
    "        print('')\n",
    "print(type(word_vectors))\n",
    "dump_embedding()\n",
    "print ('The vocabulary is stored as a standard {} of len {}'.format(type(word_vectors.vocab), len(word_vectors.vocab)))\n",
    "print ('There is also a {}, also of len {}'.format(type(word_vectors.index2word), len(word_vectors.index2word)))\n",
    "print ('Here are the first 10 entries:  {}'.format(word_vectors.index2word[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke-trained, near woman?\n",
      "[(u'man', 0.8185614347457886), (u'gentleman', 0.5897939205169678), (u'woman', 0.5865741968154907), (u'mans', 0.5807508230209351), (u'lady', 0.5801395177841187), (u'businessman', 0.5530569553375244), (u'guy', 0.5443193912506104), (u'husband', 0.4750097990036011), (u'madam', 0.47310683131217957), (u'smiles', 0.4672502279281616)]\n",
      "Google-trained, near woman?\n",
      "[(u'woman', 0.718680202960968), (u'man', 0.6557512283325195), (u'girl', 0.5882835388183594), (u'lady', 0.5754351615905762), (u'teenage_girl', 0.5700528621673584), (u'teenager', 0.5378326177597046), (u'schoolgirl', 0.497780978679657), (u'policewoman', 0.49065014719963074), (u'blonde', 0.4870774447917938), (u'redhead', 0.4778464436531067)]\n"
     ]
    }
   ],
   "source": [
    "femininity = word_vectors['king'] - word_vectors['queen']\n",
    "near_woman = word_vectors['man'] - femininity\n",
    "print('Joke-trained, near woman?')\n",
    "print(word_vectors.most_similar(positive=[near_woman]))\n",
    "\n",
    "femininity = google_vectors['king'] - google_vectors['queen']\n",
    "near_woman = google_vectors['man'] - femininity\n",
    "print('Google-trained, near woman?')\n",
    "print(google_vectors.most_similar(positive=[near_woman]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "def doc2avgcbow(sentence):\n",
    "    # this analyzer from sklearn tokenizes including stop words and lower case\n",
    "    analyze = CountVectorizer(stop_words='english').build_analyzer()\n",
    "    tokens = analyze(sentence)\n",
    "    avg_cbow = np.zeros(100)\n",
    "    N = 0\n",
    "    for token in tokens:\n",
    "        #print('   {}'.format(token))\n",
    "        if token in word_vectors.vocab:\n",
    "            N += 1\n",
    "            avg_cbow += word_vectors[token]\n",
    "    if N != 0:\n",
    "        # Some jokes contain nothing in our vocabulary.  Examples.\n",
    "        #  What do you call a shart? Woopsie Poopsie :) \n",
    "        # 1 2 3 4 5 6\n",
    "        # Who do you call when a sleepwalker injures himself? The somnambulance.\n",
    "        avg_cbow = avg_cbow/float(N)\n",
    "    return avg_cbow\n",
    "\n",
    "def avg_cbow():\n",
    "    sentences = df['title'] + ' ' + df['body']\n",
    "    for s in sentences:\n",
    "        yield doc2avgcbow(s)\n",
    "\n",
    "df['cbow'] = [f for f in avg_cbow()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9981496235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = df['cbow']\n",
    "y = pd.cut(df.score, 3, labels=['low', 'med', 'high'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train.tolist(), y_train.tolist())\n",
    "score = clf.score(X_test.tolist(), y_test.tolist())\n",
    "print('{}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
