{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jokes - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_json\n",
    "from gensim.models import word2vec\n",
    "df = read_json(\"reddit_jokes.json\")\n",
    "sentences = df['title'] + ' ' + df['body']\n",
    "sentences = [s.split() for s in sentences.tolist()]\n",
    "# this trains a cbow on sentences\n",
    "model = word2vec.Word2Vec(sentences, size=200, min_count=1)\n",
    "model.wv.save('reddit_jokes.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the non-trainable part of the model from disk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load('reddit_jokes.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the top-N most similar words. Positive words contribute positively towards the similarity, negative words negatively.\n",
    "\n",
    "This method computes cosine similarity between a simple mean of the projection weight vectors of the given words and the vectors for each word in the model. The method corresponds to the word-analogy and distance scripts in the original word2vec implementation.word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'nurse', 0.5612561702728271),\n",
       " (u'widow', 0.5456728339195251),\n",
       " (u'fairy', 0.5337789058685303),\n",
       " (u'Queen', 0.5314375758171082),\n",
       " (u'mayor', 0.530626118183136),\n",
       " (u'chief', 0.5280809998512268),\n",
       " (u'bishop', 0.5274410247802734),\n",
       " (u'queen', 0.5271376371383667),\n",
       " (u'judge', 0.5261480808258057),\n",
       " (u'minister', 0.5257188677787781)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of our vectorization (compare to the ctor for Word2Vec): 200\n",
      "First 12 features:  [ 0.70628828  0.21708411  0.15808964 -0.04344378  1.16073442 -0.19331229\n",
      "  0.3006101   0.07726071  1.03134263 -0.15870611 -0.45126703  0.55619991]\n"
     ]
    }
   ],
   "source": [
    "# Show me the vectorization of the word 'king'\n",
    "vKing = word_vectors['king']\n",
    "print ('Length of our vectorization (compare to the ctor for Word2Vec): {}'.format(len(vKing)))\n",
    "print ('First 12 features:  {}'.format(vKing[:12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
